Only in eval_code_modified: __pycache__
diff -r eval_code/eval.py eval_code_modified/eval.py
17c17
<     checkpoint = './models/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth'
---
>     checkpoint = 'eval_code/models/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth'
23,24c23,24
<     cfgfile = "models/yolov4.cfg"
<     weightfile = "models/yolov4.weights"
---
>     cfgfile = "eval_code/models/yolov4.cfg"
>     weightfile = "eval_code/models/yolov4.weights"
36c36
<         img_file_dir2 = selected_path.replace('_p', '')  # clean
---
>         img_file_dir2 = 'eval_code/select1000_new/images/'  # clean
54c54,55
<         assert len(boxes0) != 0
---
>         # assert len(boxes0) != 0
>         print(f'{len(boxes0)} : {len(boxes1)}')
56c57
<         bb_score = 1 - min(len(boxes0), len(boxes1))/len(boxes0)
---
>         bb_score = 1 - min(len(boxes0), len(boxes1))/max(len(boxes0), 1)
74c75
<         img_path0 = os.path.join(selected_path.replace('_p', ''), img_name)
---
>         img_path0 = os.path.join('eval_code/select1000_new/images/', img_name)
86a88
>             print(f'patch_number: {patch_number}')
90a93
>             print(f'patch_number: {patch_number}')
94a98
>             print(f'total_area_rate: {total_area_rate}')
104,105c108,110
<     ones = torch.cuda.FloatTensor(input_img[0].size()).fill_(1)
<     zeros = torch.cuda.FloatTensor(input_img[0].size()).fill_(0)
---
>     input_img_new = (input_img[0]+input_img[1]+input_img[2])
>     ones = torch.cuda.FloatTensor(input_img_new.size()).fill_(1)
>     zeros = torch.cuda.FloatTensor(input_img_new.size()).fill_(0)
107,110c112,113
<     input_img_tmp2 = torch.where((input_img[0] != 0), ones, zeros) + \
<                      torch.where((input_img[1] != 0), ones, zeros) + \
<                      torch.where((input_img[2] != 0), ones, zeros)
<     input_map_new = torch.where(input_img_tmp2 > 0, ones, zeros)
---
>     whole_size = input_img_new.shape[0]*input_img_new.shape[1]
>     input_map_new = torch.where((input_img_new != 0), ones, zeros)
112d114
<     whole_size = input_map_new.shape[0] * input_map_new.shape[1]
124,125c126,127
< 
<     area_score = 2 - float(total_area_rate / max_total_area_rate)
---
>     
>     area_score = 2 - float(total_area_rate/max_total_area_rate)
diff -r eval_code/infer.py eval_code_modified/infer.py
17c17
<     img_dir2 = img_dir.replace('_p', '')
---
>     img_dir2 = 'eval_code/select1000_new/images/'
27c27
<             bbox_results, _ = result_p
---
>             bbox_results, *_ = result_p
29c29
<             bbox_results, _ = result_c
---
>             bbox_results, *_ = result_c
33,34c33,37
<         result_p = np.concatenate(result_p)
<         result_c = np.concatenate(result_c)
---
> 
>         result_p = np.concatenate(result_p[0])
>         result_c = np.concatenate(result_c[0])
> 
>     
43,44c46,47
<             print('i=', ik)
<             print(file_name)
---
>             # print('i=', ik)
>             # print(file_name)
49a53
>         print(f'{result_above_confidence_num_c} : {result_above_confidence_num_p}')
Only in eval_code_modified/select1000_new: images
Only in eval_code_modified/tool: __pycache__
diff -r eval_code/tool/darknet2pytorch.py eval_code_modified/tool/darknet2pytorch.py
103c103
<         super(Darknet, self).__init__()
---
>         super().__init__()
188c188
<             return out_boxes
---
>             return out_boxes, outputs
193a194,205
>         class MaxPoolGrad2d(torch.nn.Module):
>             def __init__(self, **kargs):
>                 super().__init__()
>                 self.kernel_size = kargs['kernel_size']
>                 self.maxpool = torch.nn.MaxPool2d(**kargs)
>                 self.avgpool = torch.nn.AvgPool2d(**kargs)
> 
>             def forward(self,x):
>                 y = self.maxpool(x)
>                 z = self.avgpool(x)
>                 return y.detach() - z.detach() + z
>         
240c252,255
<                 model = nn.MaxPool2d(kernel_size=pool_size, stride=stride, padding=pool_size//2)
---
>                 if pool_size != 0:
>                     model = MaxPoolGrad2d(kernel_size=pool_size, stride=stride, padding=pool_size//2)
>                 else:
>                     model = nn.MaxPool2d(kernel_size=pool_size, stride=stride, padding=pool_size//2)
Only in eval_code_modified/utils: __pycache__
diff -r eval_code/utils/utils.py eval_code_modified/utils/utils.py
151c151,152
<     cls_confs = torch.nn.Softmax()(Variable(output[5:5 + num_classes].transpose(0, 1))).data
---
> #     cls_confs = torch.nn.Softmax()(Variable(output[5:5 + num_classes].transpose(0, 1))).data
>     cls_confs = torch.nn.Softmax()(output[5:5 + num_classes].transpose(0, 1))
207,289c208
< def get_region_boxes1(output, conf_thresh, num_classes, anchors, num_anchors, only_objectness=1, validation=False):
<     anchor_step = len(anchors) // num_anchors
<     if len(output.shape) == 3:
<         output = np.expand_dims(output, axis=0)
<     batch = output.shape[0]
<     assert (output.shape[1] == (5 + num_classes) * num_anchors)
<     h = output.shape[2]
<     w = output.shape[3]
< 
<     t0 = time.time()
<     all_boxes = []
<     output = output.reshape(batch * num_anchors, 5 + num_classes, h * w).transpose((1, 0, 2)).reshape(
<         5 + num_classes,
<         batch * num_anchors * h * w)
< 
<     grid_x = np.expand_dims(np.expand_dims(np.linspace(0, w - 1, w), axis=0).repeat(h, 0), axis=0).repeat(
<         batch * num_anchors, axis=0).reshape(
<         batch * num_anchors * h * w)
<     grid_y = np.expand_dims(np.expand_dims(np.linspace(0, h - 1, h), axis=0).repeat(w, 0).T, axis=0).repeat(
<         batch * num_anchors, axis=0).reshape(
<         batch * num_anchors * h * w)
< 
<     xs = sigmoid(output[0]) + grid_x
<     ys = sigmoid(output[1]) + grid_y
< 
<     anchor_w = np.array(anchors).reshape((num_anchors, anchor_step))[:, 0]
<     anchor_h = np.array(anchors).reshape((num_anchors, anchor_step))[:, 1]
<     anchor_w = np.expand_dims(np.expand_dims(anchor_w, axis=1).repeat(batch, 1), axis=2) \
<         .repeat(h * w, axis=2).transpose(1, 0, 2).reshape(batch * num_anchors * h * w)
<     anchor_h = np.expand_dims(np.expand_dims(anchor_h, axis=1).repeat(batch, 1), axis=2) \
<         .repeat(h * w, axis=2).transpose(1, 0, 2).reshape(batch * num_anchors * h * w)
<     ws = np.exp(output[2]) * anchor_w
<     hs = np.exp(output[3]) * anchor_h
< 
<     det_confs = sigmoid(output[4])
< 
<     cls_confs = softmax(output[5:5 + num_classes].transpose(1, 0))
<     cls_max_confs = np.max(cls_confs, 1)
<     cls_max_ids = np.argmax(cls_confs, 1)
<     t1 = time.time()
< 
<     sz_hw = h * w
<     sz_hwa = sz_hw * num_anchors
<     t2 = time.time()
<     for b in range(batch):
<         boxes = []
<         for cy in range(h):
<             for cx in range(w):
<                 for i in range(num_anchors):
<                     ind = b * sz_hwa + i * sz_hw + cy * w + cx
<                     det_conf = det_confs[ind]
<                     if only_objectness:
<                         conf = det_confs[ind]
<                     else:
<                         conf = det_confs[ind] * cls_max_confs[ind]
< 
<                     if conf > conf_thresh:
<                         bcx = xs[ind]
<                         bcy = ys[ind]
<                         bw = ws[ind]
<                         bh = hs[ind]
<                         cls_max_conf = cls_max_confs[ind]
<                         cls_max_id = cls_max_ids[ind]
<                         box = [bcx / w, bcy / h, bw / w, bh / h, det_conf, cls_max_conf, cls_max_id]
<                         if (not only_objectness) and validation:
<                             for c in range(num_classes):
<                                 tmp_conf = cls_confs[ind][c]
<                                 if c != cls_max_id and det_confs[ind] * tmp_conf > conf_thresh:
<                                     box.append(tmp_conf)
<                                     box.append(c)
<                         boxes.append(box)
<         all_boxes.append(boxes)
<     t3 = time.time()
<     if False:
<         print('---------------------------------')
<         print('matrix computation : %f' % (t1 - t0))
<         print('        gpu to cpu : %f' % (t2 - t1))
<         print('      boxes filter : %f' % (t3 - t2))
<         print('---------------------------------')
<     return all_boxes
< 
< 
< def plot_boxes_cv2(img, boxes, savename=None, class_names=None, color=None):
---
> def plot_boxes_cv2(img, boxes, savename=None, class_names=True, color=None):
302a222
>     bx = []
317,326c237,249
<             print('%s: %f' % (class_names[cls_id], cls_conf))
<             classes = len(class_names)
<             offset = cls_id * 123457 % classes
<             red = get_color(2, offset, classes)
<             green = get_color(1, offset, classes)
<             blue = get_color(0, offset, classes)
<             if color is None:
<                 rgb = (red, green, blue)
<             img = cv2.putText(img, class_names[cls_id], (x1, y1), cv2.FONT_HERSHEY_SIMPLEX, 1.2, rgb, 1)
<         img = cv2.rectangle(img, (x1, y1), (x2, y2), rgb, 1)
---
>         
> #             classes = len(class_names)
> #             offset = cls_id * 123457 % classes
> #             red = get_color(2, offset, classes)
> #             green = get_color(1, offset, classes)
> #             blue = get_color(0, offset, classes)
> #             if color is None:
> #                 rgb = (red, green, blue)
> #             img = cv2.putText(img, str(cls_conf), (x1, y1), cv2.FONT_HERSHEY_SIMPLEX, 3, rgb, 3)
> 
>         img = cv2.rectangle(img, (x1, y1), (x2, y2), (0, 0, 255), 3)
>         img = cv2.putText(img, f'{box[4]:.2f}', (x1, y1 + 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 3)
>         bx.append([x1, y1, x2, y2])
330c253
<     return img
---
>     return img, bx
395c318
< def do_detect(model, img, conf_thresh, nms_thresh, use_cuda=1):
---
> def do_detect(model, img, conf_thresh, nms_thresh, use_cuda=1, output_img=False):
420a344
>     img.requires_grad = True
423c347
<     list_boxes = model(img)
---
>     list_boxes, _ = model(img)
438,439c362,363
<         boxes.append(get_region_boxes1(list_boxes[i].data.cpu().numpy(), conf_thresh, 80, masked_anchors, len(anchor_masks[i])))
<         # boxes.append(get_region_boxes(list_boxes[i], 0.6, 80, masked_anchors, len(anchor_masks[i])))
---
> #         boxes.append(get_region_boxes1(list_boxes[i].data.cpu().numpy(), conf_thresh, 80, masked_anchors, len(anchor_masks[i])))
>         boxes.append(get_region_boxes(list_boxes[i], nms_thresh, 80, masked_anchors, len(anchor_masks[i])))
449a374,434
>     t4 = time.time()
> 
>     if False:
>         print('-----------------------------------')
>         print(' image to tensor : %f' % (t1 - t0))
>         print('  tensor to cuda : %f' % (t2 - t1))
>         print('         predict : %f' % (t3 - t2))
>         print('             nms : %f' % (t4 - t3))
>         print('           total : %f' % (t4 - t0))
>         print('-----------------------------------')
>     
>     if output_img == False:
>         return boxes
>     else:
>         return img, boxes
> 
> 
> def do_detect_all(model, img, conf_thresh, nms_thresh, use_cuda=1):
>     model.eval()
>     t0 = time.time()
> 
>     if isinstance(img, Image.Image):
>         width = img.width
>         height = img.height
>         img = torch.ByteTensor(torch.ByteStorage.from_buffer(img.tobytes()))
>         img = img.view(height, width, 3).transpose(0, 1).transpose(0, 2).contiguous()
>         img = img.view(1, 3, height, width)
>         img = img.float().div(255.0)
>     elif type(img) == np.ndarray and len(img.shape) == 3:  # cv2 image
>         img = torch.from_numpy(img.transpose(2, 0, 1)).float().div(255.0).unsqueeze(0)
>     elif type(img) == np.ndarray and len(img.shape) == 4:
>         img = torch.from_numpy(img.transpose(0, 3, 1, 2)).float().div(255.0)
>     elif type(img) == torch.Tensor and len(img.shape) == 4:
>         img = img
>     else:
>         print("unknow image type")
>         exit(-1)
> 
>     t1 = time.time()
> 
>     if use_cuda:
>         img = img.cuda()
>     t2 = time.time()
> 
>     list_boxes, _ = model(img)
> 
>     anchors = [12, 16, 19, 36, 40, 28, 36, 75, 76, 55, 72, 146, 142, 110, 192, 243, 459, 401]
>     num_anchors = 9
>     anchor_masks = [[0, 1, 2], [3, 4, 5], [6, 7, 8]]
>     strides = [8, 16, 32]
>     anchor_step = len(anchors) // num_anchors
>     boxes = []
>     for i in range(3):
>         masked_anchors = []
>         for m in anchor_masks[i]:
>             masked_anchors += anchors[m * anchor_step:(m + 1) * anchor_step]
>         masked_anchors = [anchor / strides[i] for anchor in masked_anchors]
>         # temp = get_region_boxes1(list_boxes[i].data.numpy(), conf_thresh, 80, masked_anchors, len(anchor_masks[i]))
>         # boxes.append(temp)
> #         boxes.append(get_region_boxes1(list_boxes[i].data.cpu().numpy(), conf_thresh, 80, masked_anchors, len(anchor_masks[i])))
>         boxes.extend(get_region_boxes_all(list_boxes[i], conf_thresh, 80, masked_anchors, len(anchor_masks[i])))
